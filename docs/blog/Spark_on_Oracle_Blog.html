<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-03-23 Tue 15:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Spark on your Oracle Data Warehouse</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Harish Butani" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/hideshow.css"/>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/bigblow.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/hideshow.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<style> #content{max-width:1000px;}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Spark on your Oracle Data Warehouse</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org9dd96eb">1. Background and Motivation</a>
<ul>
<li><a href="#org52ba6fe">1.1. Reality of these deployments</a></li>
<li><a href="#org6549c15">1.2. For the Oracle customer</a></li>
</ul>
</li>
<li><a href="#orgd406719">2. Our Solution: An integrated platform</a></li>
<li><a href="#org11a7f53">3. Summary of Spark Extensions</a>
<ul>
<li><a href="#orgcedb294">3.1. Catalog Integration</a></li>
<li><a href="#org81e4f09">3.2. SQL Translation and Pushdown</a></li>
<li><a href="#orgf677f04">3.3. Language Integration</a>
<ul>
<li><a href="#org4116981">3.3.1. Spark SQL Macros</a></li>
</ul>
</li>
<li><a href="#orgf7584ec">3.4. Runtime Integration</a></li>
</ul>
</li>
<li><a href="#org2757e59">4. A Peek under the covers</a>
<ul>
<li><a href="#orgf9b87f7">4.1. Catalog Integration</a></li>
<li><a href="#org1798af2">4.2. TPCDS Queries</a>
<ul>
<li><a href="#org5ddc26e">4.2.1. Q1</a></li>
<li><a href="#orgf0b558e">4.2.2. Pushdown plan for Q5, Q69</a></li>
</ul>
</li>
<li><a href="#orgf2e0e50">4.3. Registration and Use of Oracle Functions(Row and Aggregate) in Spark SQL</a></li>
<li><a href="#org851852d">4.4. Spark SQL Macros</a>
<ul>
<li><a href="#org26a6eb8">4.4.1. Defining Macros</a></li>
<li><a href="#orgd1bab67">4.4.2. Tax and Discount Calculation</a></li>
<li><a href="#org1157398">4.4.3. Query Plan and execution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S9J2CXZFNT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S9J2CXZFNT');
</script>

<div id="outline-container-org9dd96eb" class="outline-2">
<h2 id="org9dd96eb"><span class="section-number-2">1</span> Background and Motivation</h2>
<div class="outline-text-2" id="text-1">
<p>
Figure <a href="#org5c1f287">1</a> depicts a not uncommon
deployment for an Enterprise that uses both open source systems and an Oracle Warehouse.
Enterprises are deploying some of their production workloads on open source technologies
 with the current favorite being <a href="https://spark.apache.org/">Apache Spark</a>.
The Oracle Warehouse and open source systems typically share a common
storage fabric, typically an object store like Amazon S3.
Some of the  motives driving such a move are:
</p>
<ul class="org-ul">
<li>newer Applications utilize capabilities such as machine learning and low-latency ingest;
open source technologies like <a href="https://spark.apache.org/">Apache Spark</a>, <a href="https://pulsar.apache.org/">Apache Pulsar</a>, <a href="https://www.tensorflow.org/">TensorFlow</a> provide
deep and fast evolving capabilities in these areas.</li>
<li>the open source community has done a good job of catering to
developers; making it quite easy for
small teams to fairly quickly develop and deploy point solutions.</li>
<li>price performance metrics of open source systems when dealing with 'Big Data'
are  perceived to be superior and an open source strategy is preferred
nowadays to avoid vendor lock in.</li>
</ul>


<div id="org5c1f287" class="figure">
<p><img src="./images/entDep.png" alt="entDep.png" width="70%" />
</p>
<p><span class="figure-number">Figure 1: </span>Enterprise Data Lake</p>
</div>

<p>
The promise of such a 'Enterprise Data Lake' deployment is that it provides a <b>best of
both worlds:</b> enabling newer applications to leverage open source stacks without
disrupting traditional production workloads such as Enterprise Performance,
Business Intelligence Dashboards and Reports, Financial Planning etc.
</p>
</div>

<div id="outline-container-org52ba6fe" class="outline-3">
<h3 id="org52ba6fe"><span class="section-number-3">1.1</span> Reality of these deployments</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Even though open source systems share the storage fabric with the Oracle
warehouse, these systems
<b>mostly run as silos</b>: each system aspires to be a full-featured data platform for
their developer communities. Coexistence with other platforms is an after
thought, and mostly handled with  a <b>'connector' story</b>. Connectors at best help
reduce data movement between the systems; even this claim has holes and in reality many problems are
left unsolved such as coordination and scheduling of tasks across these systems,
co-location of runtime resources, efficient data value mapping(not withstanding the promise of <a href="https://arrow.apache.org/">Apache Arrow</a>),
and deep translation and pushdown of SQL.
</p>



<div id="org0e199a3" class="figure">
<p><img src="./images/conn.png" alt="conn.png" width="90%" />
</p>
<p><span class="figure-number">Figure 2: </span>Non-optimal Data movement and Work coordination between systems</p>
</div>
</div>
</div>

<div id="outline-container-org6549c15" class="outline-3">
<h3 id="org6549c15"><span class="section-number-3">1.2</span> For the Oracle customer</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Within the space of multi-system deployments there is a segment of customer whose
<b>data platform is centered around an Oracle Data Warehouse</b>, but who still deploys
some Data Applications using Apache Spark. Such customers are often surprised by
the <b>significant increase in operational cost of running  Spark and Oracle</b> in
production:
</p>

<ul class="org-ul">
<li>In areas such as Workload management, Query optimization, Data management,
Data security Oracle customers are surprised by missing capabilities in Spark and how
much manual labor is needed to compensate for this. They are surprised by the
investment needed in having to employ a fairly
big Data Engineering and Platform team for Spark.</li>
<li>To meet operational SLAs Spark clusters end by being over-provisioned; to quote
<a href="https://www.datanami.com/2020/07/29/big-data-apps-wasting-billions-in-the-cloud/">Pepperdata</a> "optimization of cloud systems can 'win back' about 50% of task
hours&#x2026; larger organizations can save as much as <code>$7.9</code> million a year".</li>
</ul>


<div id="org851e9bb" class="figure">
<p><img src="./images/statusquo.png" alt="statusquo.png" width="90%" />
</p>
<p><span class="figure-number">Figure 3: </span>Reality: Oracle Customers and Spark</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd406719" class="outline-2">
<h2 id="orgd406719"><span class="section-number-2">2</span> Our Solution: An integrated platform</h2>
<div class="outline-text-2" id="text-2">
<p>
We provide Oracle Customers with a <b>more integrated environment</b> where
Data management and operational tasks are mostly handled in the Oracle
Warehouse. At the same time App. developers are abstracted away from the inner workings of the
platform, given them  more fluidity in the choices of Language and  Functions for their
Applications. This fits into Oracle's convergence story of consistent data management,
security, operations across all kinds of data processing tasks. 
</p>

<p>
There is <b>no impact on Applications written against the Spark programming
model.</b> Customers would <b>deploy Spark in the usual ways</b>: local-mode, cluster-mode,
server-less etc; with an additional extension jar provided by oracle and
instructions on how to configure the Oracle extensions. Since lesser
amounts of processing needs to be done in Spark, and Oracle is leveraged for
Administration and Data Management it <b>opens the door to operate smaller and
operationally simpler Spark clusters.</b>  
</p>


<div id="orgcd0f3f3" class="figure">
<p><img src="./images/spark_on_ora_goals.png" alt="spark_on_ora_goals.png" width="90%" />
</p>
<p><span class="figure-number">Figure 4: </span>An integrated Oracle and Spark architecture</p>
</div>

<p>
In order to make this happen, we <b>provide the following new capabilities:</b>
</p>
<dl class="org-dl">
<dt>Catalog Integration</dt><dd>The Oracle Data Dictionary will be the source of truth for
all metadata. We extend the Spark Catalog to be integrated with the Oracle
Dictionary.</dd>
<dt>Program Translation and Pushdown</dt><dd>by far this is the <b>most important
capability</b> that involves
the translation of Spark SQL into Oracle SQL(and PL-SQL) for pipelines when the
data is residing/managed in Oracle. This <i>applies to more than raw table
data</i> stored in Oracle to
data managed in Oracle via  such things as <a href="https://docs.oracle.com/en/database/oracle/oracle-database/21/dwhsg/basic-materialized-views.html#GUID-A7AE8E5D-68A5-4519-81EB-252EAAF0ADFF">Materialized Views</a>,  <a href="https://docs.oracle.com/en/database/oracle/oracle-database/21/dwhsg/part-analytic-views.html#GUID-AC5ACD4F-69F3-4C32-B7A1-EABF93639BEC">Analytic
Views</a> and <a href="https://docs.oracle.com/en/database/oracle/oracle-database/21/inmem/index.html">In-Memory Option</a>. <b>Pushing
the processing to where the data is, even for complex data pipelines provides</b>
<b>the biggest differentiation from 'connector' architectures.</b></dd>
<dt>Language Integration</dt><dd>Language Integration encompasses capabilities that
extend Apache Spark such that on the one hand <b>native Oracle functions and
data structures</b> are usable in Spark programs and on the other hand the
translation of <b>a subset of custom scala programs into equivalent oracle
sql/pl-sql</b>, so that larger parts of Spark pipelines are pushed down to the
Oracle DB for processing.</dd>
<dt>Runtime Integration</dt><dd>Even with Pushdown and Language Integration there are
many pipelines that will contain Spark specific program code; this would
trigger large amounts of data to be streamed out of Oracle to
Spark executors. A <b>Spark co-processor</b> either co-located with Oracle
instances  or embedded in them would allow for limited Spark pipelines to
be shipped to the Oracle instances for execution.</dd>
</dl>


<div id="orgaeb25b9" class="figure">
<p><img src="./images/spark_on_ora_goals2.png" alt="spark_on_ora_goals2.png" width="70%" />
</p>
<p><span class="figure-number">Figure 5: </span>Capabilities of an integrated system</p>
</div>
</div>
</div>

<div id="outline-container-org11a7f53" class="outline-2">
<h2 id="org11a7f53"><span class="section-number-2">3</span> Summary of Spark Extensions</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgcedb294" class="outline-3">
<h3 id="orgcedb294"><span class="section-number-3">3.1</span> Catalog Integration</h3>
<div class="outline-text-3" id="text-3-1">
<p>
We leverage the <a href="https://databricks.com/session/apache-spark-data-source-v2">Catalog Integration and DataSources v2 API</a>
in Apache Spark 3.0 to provide a custom plug-in Catalog that
integrates with the Oracle Database Data Dictionary.  A common catalog implies all
persistent data, including data residing  in an Object Store is
represented in the Oracle Data Dictionary. Hence both systems
have a consistent view of the Data Warehouse; a common and
consistent enforcement of role and data based security, a common metadata and
data consistency model can be applied.
</p>
</div>
</div>

<div id="outline-container-org81e4f09" class="outline-3">
<h3 id="org81e4f09"><span class="section-number-3">3.2</span> SQL Translation and Pushdown</h3>
<div class="outline-text-3" id="text-3-2">
<p>
These are based on techniques developed over many years to extend the Spark
Planner with custom Query Optimizations. See <a href="https://github.com/SparklineData/spark-druid-olap">Spark Druid OLAP</a> and
<a href="https://medium.com/@rhbutani/https-medium-com-oracle-snap-benefits-of-bi-semantics-in-spark-sql-a-view-through-the-tpcds-benchmark-5cca8d6d25d2">Sparklinedata BI Stack on Spark</a>. From the early days it has been possible to
extend Spark's  Logical and Physical planners. By rewriting Spark Plans we can
go way beyond what most Spark 'connectors' provide(that is  pushing projections
and filters to the underlying system); <b>We are able to entirely push  complex
analysis pipelines containing all the analytical functions and operators</b>
<b>of Spark SQL.</b> 
</p>

<p>
<b>For the <a href="http://www.tpc.org/tpcds/">TPCDS benchmark</a> we are able to completely pushed down over 90 of the 99 queries.</b>
For example Figure  <a href="#org0e65d1e">6</a> shows the non-pushdown vs pushdown plan for TPCDS query
q1. We provide more details about TPCDS queries in the <a href="#org1798af2">TPCDS Queries section</a>.
</p>


<div id="org0e65d1e" class="figure">
<p><img src="./images/tpcds_q1.png" alt="tpcds_q1.png" width="60%" height="60%" />
</p>
<p><span class="figure-number">Figure 6: </span>TPCDS Query Q1: non-pushdown vs pushdown plan.</p>
</div>


<p>
When a pushdown query returns a lot of data, the single pipe between a Spark task and the
database instance could become a bottleneck of query execution. The <b>Query
Splitting</b> feature attempts to split an oracle pushdown query into a set of
queries such that the union-all of the results is the same as the original query
result.  The query can be split by <b>Input Table(s) partitions or blocks</b> or by
<b>output Result-Set row ranges.</b>
</p>


<div id="orgca6f7a6" class="figure">
<p><img src="./images/parallelDataMove.png" alt="parallelDataMove.png" width="60%" />
</p>
<p><span class="figure-number">Figure 7: </span>Query Splitting</p>
</div>

<p>
For DML operations we integrate into <a href="https://databricks.com/session/apache-spark-data-source-v2">Spark's Datasources v2</a> API to provide
transactionally consistent DML: during spark job execution, all other jobs will
see the state of the table as of the start of the job and we don't have to prevent
concurrent dml jobs on a table. On success, the destination table will  be in a
state that matches some serialization of the jobs. Under the covers data is first
written to Temp. tables in Oracle before being merged into the destination table
on commit.
</p>


<div id="org9dfa824" class="figure">
<p><img src="./images/basicInsertFlow.png" alt="basicInsertFlow.png" width="90%" />
</p>
<p><span class="figure-number">Figure 8: </span>Insert Flow</p>
</div>
</div>
</div>

<div id="outline-container-orgf677f04" class="outline-3">
<h3 id="orgf677f04"><span class="section-number-3">3.3</span> Language Integration</h3>
<div class="outline-text-3" id="text-3-3">
<p>
We leverage the ability to register of Custom Functions in Spark to enable the
registration of  Oracle SQL functions into Spark. These functions can then be
used in Spark-SQL on Oracle tables. Our Planner extensions translate these into
equivalent Oracle SQL for execution. 
</p>

<p>
We are also investigating surfacing Oracle ML and Geo-spatial capabilities into the
Spark programming model. 
</p>
</div>

<div id="outline-container-org4116981" class="outline-4">
<h4 id="org4116981"><span class="section-number-4">3.3.1</span> Spark SQL Macros</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
Spark provides the ability to register custom functions written in Scala.
Under the covers an <code>Invoke Catalyst Expression</code> is associated with the function
name in Spark's <code>Function Registry</code>. These functions are usable in Spark-SQL
just like built-in functions. At runtime an Invoke Catalyst Expression runs
the associated function body.
</p>


<div id="orge67da0a" class="figure">
<p><img src="./images/spark_cust_fn.png" alt="spark_cust_fn.png" width="90%" />
</p>
<p><span class="figure-number">Figure 9: </span>Spark Function Registration and Execution</p>
</div>

<p>
We have developed the <b>Spark SQL Macros</b> capability that provides the ability to register
custom scala functions into a Spark Session just like the custom UDF
Registration capability of Spark. The difference being that the <b>SQL Macros
registration mechanism attempts to translate the function body to an equivalent</b>
<b>Spark catalyst Expression</b> with holes(MarcroArg catalyst expressions). Under
the covers SQLMacro is a <a href="https://docs.scala-lang.org/overviews/macros/overview.html">set of scala blackbox macros</a> that attempt to rewrite
the scala function body AST into an equivalent catalyst Expression. There are
2 big benefits of doing this:
</p>
<ul class="org-ul">
<li>better performance. Since at runtime we avoid the SerDe cost at the function boundary.</li>
<li>Since the physical plan has native catalyst expressions more optimizations are possible.
In the case of Oracle pushdown <b>we are able to pushdown the catalyst
expressions to Oracle SQL, avoiding streaming data out of Oracle even in the</b>
<b>presence of these kinds of custom UDFs.</b></li>
</ul>


<div id="org80b7fc2" class="figure">
<p><img src="./images/spark-macro-reg.png" alt="spark-macro-reg.png" width="90%" />
</p>
<p><span class="figure-number">Figure 10: </span>Spark SQL Macro Registration and Execution</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf7584ec" class="outline-3">
<h3 id="orgf7584ec"><span class="section-number-3">3.4</span> Runtime Integration</h3>
<div class="outline-text-3" id="text-3-4">
<p>
We are investigating the concept of a <b>Spark Co-Processor</b>. A co-processor will
be a <i>very limited and isolated Spark environment</i> whose only capability is to
run a given  Spark operator pipeline in a single Task(the Pipeline 
cannot contain Shuffle operations). Co-processors are unaware of cluster topology
or the data dictionary.  They are dumb processing units for running custom scala code that
apply row/row-group transformations. A Co-Processor could be an isolated process
co-located with Oracle Instance processes or it could be embedded inside an Oracle
process along the lines of <a href="https://medium.com/graalvm/bringing-modern-programming-languages-to-the-oracle-database-with-graalvm-80914d0c0167">how a javascript engine is embedded inside an
Oracle process</a>.
</p>

<p>
Logically a Spark Plan will get translated into Oracle SQL
query blocks connected by calls to a Partitioned Table function. At runtime,
this function will be the bridge to the co-processor in
the Oracle Operator pipeline. Each Table function invocation will stream its
input partition to and from the co-processor; it will coordinate setting up and
shutting down the Spark pipeline and executing the pipeline on data chunks. Data
Parallelism between Oracle and Spark will be controlled by using <a href="https://docs.oracle.com/en/database/oracle/oracle-database/19/addci/using-pipelined-and-parallel-table-functions.html#GUID-EFB94CFB-3E44-4236-B490-ADBB480C94D4">Pipelined
Parallel Table Function</a> mechanics specifically Order By/Cluster By clauses in
its definition.
</p>


<div id="org38a0c2c" class="figure">
<p><img src="./images/log_tbl_fun.png" alt="log_tbl_fun.png" width="90%" />
</p>
<p><span class="figure-number">Figure 11: </span>Logical Oracle Plan structure with Table function calls</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org2757e59" class="outline-2">
<h2 id="org2757e59"><span class="section-number-2">4</span> A Peek under the covers</h2>
<div class="outline-text-2" id="text-4">
<p>
We walk through a sample session of this integrated environment and show some
examples. The Oracle Instance is loaded with the <a href="http://www.tpc.org/tpcds/">TPCDS dataset</a> and we connect to it via
<i>Spark Shell</i>. The Spark Configuration for this environment has extra settings
such as the connection information and query pushdown instructions. 
</p>

<div class="org-src-container">
<pre class="src src-text"># Oracle Catalog

# enable Spark Oracle extensions
spark.sql.extensions=org.apache.spark.sql.oracle.SparkSessionExtensions
spark.kryo.registrator=org.apache.spark.sql.connector.catalog.oracle.OraKryoRegistrator

# enable the Oracle Catalog integration
spark.sql.catalog.oracle=org.apache.spark.sql.connector.catalog.oracle.OracleCatalog

# oracle instance connection information
spark.sql.catalog.oracle.url=dbc:oracle:thin:@den02ads:1531/cdb1_pdb7.dev.us.oracle.com
spark.sql.catalog.oracle.user=tpcds
spark.sql.catalog.oracle.password=...

# oracle sql logging and jdbc fetchsize
spark.sql.catalog.oracle.log_and_time_sql.enabled=true
spark.sql.catalog.oracle.log_and_time_sql.log_level=info
spark.sql.catalog.oracle.fetchSize=5000

# Query pushdown
spark.sql.oracle.enable.pushdown=true

# Parallelize data movement.
spark.sql.oracle.enable.querysplitting=true
spark.sql.oracle.querysplit.target=1Mb
spark.sql.oracle.querysplit.maxfetch.rounds=0.5
</pre>
</div>

<p>
The Spark shell is then started in the normal way; for example in local mode one
could issue: <code>bin/spark-shell --properties-file spark.oracle.properties --master
local[*]</code>.
</p>
</div>

<div id="outline-container-orgf9b87f7" class="outline-3">
<h3 id="orgf9b87f7"><span class="section-number-3">4.1</span> Catalog Integration</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The user can browse the Oracle catalog and describe individual tables.
</p>


<div class="org-src-container">
<pre class="src src-scala">sql("use oracle").show()
sql("show tables").show(10000, false)
sql("describe store_sales").show(10000, false)
</pre>
</div>


<div id="orgf3edb0d" class="figure">
<p><img src="./images/catalog.png" alt="catalog.png" width="90%" />
</p>
<p><span class="figure-number">Figure 12: </span>Catalog Commands</p>
</div>

<p>
Oracle has many partitioning schemes such as  <b>RANGE, LIST, INTERVAL, and HASH</b>
which don't map very well to Spark's value based partitioning scheme; the output
Spark SQL <code>show partitions</code> command doesn't give the full partitioning picture.
So we have extended Spark SQL; users can issue <code>show oracle partitions</code> to
see the table partitioning details.
</p>

<div class="org-src-container">
<pre class="src src-scala">// show partitions
sql("show partitions store_sales").show(1000, false)
// spark language extension  to see oracle partitions properly
sql("show oracle partitions store_sales").show(1000, false)
</pre>
</div>
</div>
</div>



<div id="outline-container-org1798af2" class="outline-3">
<h3 id="org1798af2"><span class="section-number-3">4.2</span> TPCDS Queries</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The <a href="http://www.tpc.org/tpcds/">TPCDS benchmark</a> contains many complex analysis patterns such
<code>Star-Schema-Agg-Joins</code>, <code>Multi-Star-Schema-Analysis</code>, <code>Windowing-Functions</code>,
<code>Correlated-SubQueries</code> etc. As we have mentioned we are able to completely
pushed down over 90 of the 99 queries.
</p>
</div>

<div id="outline-container-org5ddc26e" class="outline-4">
<h4 id="org5ddc26e"><span class="section-number-4">4.2.1</span> Q1</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>Query is about identifying 'problem' customers.</li>
<li>Query involves joins, aggregates, a CTE, a subquery expression, Order By</li>
</ul>

<div class="org-src-container">
<pre class="src src-scala">val q1 = s""" with customer_total_return as
(select sr_customer_sk as ctr_customer_sk
,sr_store_sk as ctr_store_sk
,sum(SR_RETURN_AMT) as ctr_total_return
from store_returns
,date_dim
where sr_returned_date_sk = d_date_sk
and d_year =2000
group by sr_customer_sk
,sr_store_sk)
 select  c_customer_id
from customer_total_return ctr1
,store
,customer
where ctr1.ctr_total_return &gt; (select avg(ctr_total_return)*1.2
from customer_total_return ctr2
where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
and s_store_sk = ctr1.ctr_store_sk
and s_state = 'TN'
and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
 limit 100; """.stripMargin
```
</pre>
</div>

<p>
Running this query <code>sql(q1).show(1000000, false)</code> with the <code>pushdown flag on</code> pushes
the entire query to Oracle and it runs in sub-second time. Issuing a
<code>sql(s"explain oracle pushdown $q1").show(1000, false)</code>
shows the Spark Plan and the Oracle Query pushed down. (here again we have
extended Spark SQL with the <code>explain oracle pushdown</code> command which is similar
to Spark's <code>explain</code> command with the addition that we show the oracle pushdown
details). The output shows that for Spark this is a very simple Plan of a custom
<code>Scan</code> followed by a <code>Project</code>. Figure <a href="#org0e65d1e">6</a> show the plan visually.
</p>
<div class="org-src-container">
<pre class="src src-text">sql(s"explain oracle pushdown $q1").show(1000, false)

|Project (1)
+- BatchScan (2)

(2) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "C_CUSTOMER_ID"
from ( select "C_CUSTOMER_ID"
from ( select "SR_CUSTOMER_SK" AS "ctr_customer_sk", "SR_STORE_SK" AS "ctr_store_sk", SUM("SR_RETURN_AMT") AS "ctr_total_return"
from TPCDS.STORE_RETURNS  join TPCDS.DATE_DIM  on ("SR_RETURNED_DATE_SK" = "D_DATE_SK")
where ((("SR_STORE_SK" IS NOT NULL AND "SR_CUSTOMER_SK" IS NOT NULL) AND "SR_RETURNED_DATE_SK" IS NOT NULL) AND ("D_YEAR" IS NOT NULL AND ("D_YEAR" = 2000.000000000000000000)))
group by "SR_CUSTOMER_SK", "SR_STORE_SK" )  join ( select "1_sparkora", "2_sparkora"
from ( select (AVG("ctr_total_return") * 1.2000000000000000000000) AS "1_sparkora", "ctr_store_sk" AS "2_sparkora"
from ( select "SR_STORE_SK" AS "ctr_store_sk", SUM("SR_RETURN_AMT") AS "ctr_total_return"
from TPCDS.STORE_RETURNS  join TPCDS.DATE_DIM  on ("SR_RETURNED_DATE_SK" = "D_DATE_SK")
where (("SR_STORE_SK" IS NOT NULL AND ("SR_RETURNED_DATE_SK" IS NOT NULL AND "SR_RETURNED_DATE_SK" IS NOT NULL)) AND ("D_YEAR" IS NOT NULL AND ("D_YEAR" = 2000.000000000000000000)))
group by "SR_CUSTOMER_SK", "SR_STORE_SK" )
group by "ctr_store_sk" )
where "1_sparkora" IS NOT NULL )  on (("ctr_store_sk" = "2_sparkora") AND (cast("ctr_total_return" as NUMBER(38, 20)) &gt; "1_sparkora")) join TPCDS.STORE  on ("ctr_store_sk" = "S_STORE_SK") join TPCDS.CUSTOMER  on ("ctr_customer_sk" = "C_CUSTOMER_SK")
where ("ctr_total_return" IS NOT NULL AND ("S_STATE" IS NOT NULL AND ("S_STATE" = 'TN')))
order by "C_CUSTOMER_ID" ASC NULLS FIRST )
where rownum &lt;= 100
Pushdown Oracle SQL, Query Splitting details:
Query is not split
</pre>
</div>

<p>
Contrast this with the plan when <code>spark.sql.oracle.enable.pushdown=false</code>. This
setting turns of the Query rewrites, so the query plan generated and execution
is similar to what you would get with a regular 'connector' that provides a
custom Spark v2 DataSource. The Spark Plan contains Spark Operations to do the
Joins, Aggregates&#x2026; only the base Table filtering and projection is pushed to Oracle. Figure
<a href="#org0e65d1e">6</a> show the plan visually. Not surprisingly execution with pushdown off
takes considerably longer to run: 10s of seconds. 
</p>

<div class="org-src-container">
<pre class="src src-text">spark.sqlContext.setConf("spark.sql.oracle.enable.pushdown", "false")
sql(s"explain oracle pushdown $q1").show(1000, false)

|TakeOrderedAndProject (1)
+- Project (2)
   +- SortMergeJoin Inner (3)
      :- Project (4)
      :  +- SortMergeJoin Inner (5)
      :     :- Project (6)
      :     :  +- SortMergeJoin Inner (7)
      :     :     :- Filter (8)
      :     :     :  +- HashAggregate (9)
      :     :     :     +- HashAggregate (10)
      :     :     :        +- Project (11)
      :     :     :           +- SortMergeJoin Inner (12)
      :     :     :              :- Project (13)
      :     :     :              :  +- Filter (14)
      :     :     :              :     +- BatchScan (15)
      :     :     :              +- Project (16)
      :     :     :                 +- Filter (17)
      :     :     :                    +- BatchScan (18)
      :     :     +- Filter (19)
      :     :        +- HashAggregate (20)
      :     :           +- HashAggregate (21)
      :     :              +- HashAggregate (22)
      :     :                 +- HashAggregate (23)
      :     :                    +- Project (24)
      :     :                       +- SortMergeJoin Inner (25)
      :     :                          :- Project (26)
      :     :                          :  +- Filter (27)
      :     :                          :     +- BatchScan (28)
      :     :                          +- Project (29)
      :     :                             +- Filter (30)
      :     :                                +- BatchScan (31)
      :     +- Project (32)
      :        +- Filter (33)
      :           +- BatchScan (34)
      +- Project (35)
         +- BatchScan (36)

(15) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "SR_CUSTOMER_SK", "SR_STORE_SK", "SR_RETURN_AMT", "SR_RETURNED_DATE_SK"
from TPCDS.STORE_RETURNS
where ("SR_STORE_SK" IS NOT NULL AND "SR_CUSTOMER_SK" IS NOT NULL) and "SR_RETURNED_DATE_SK" IS NOT NULL
Pushdown Oracle SQL, Query Splitting details:
Query is not split
(18) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "D_DATE_SK", "D_YEAR"
from TPCDS.DATE_DIM
where ("D_YEAR" IS NOT NULL AND ("D_YEAR" = 2000.000000000000000000))
Pushdown Oracle SQL, Query Splitting details:
Query is not split
(28) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "SR_CUSTOMER_SK", "SR_STORE_SK", "SR_RETURN_AMT", "SR_RETURNED_DATE_SK"
from TPCDS.STORE_RETURNS
where "SR_STORE_SK" IS NOT NULL and ("SR_RETURNED_DATE_SK" IS NOT NULL AND "SR_RETURNED_DATE_SK" IS NOT NULL)
Pushdown Oracle SQL, Query Splitting details:
Query is not split
(31) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "D_DATE_SK", "D_YEAR"
from TPCDS.DATE_DIM
where ("D_YEAR" IS NOT NULL AND ("D_YEAR" = 2000.000000000000000000))
Pushdown Oracle SQL, Query Splitting details:
Query is not split
(34) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "S_STORE_SK", "S_STATE"
from TPCDS.STORE
where ("S_STATE" IS NOT NULL AND ("S_STATE" = 'TN'))
Pushdown Oracle SQL, Query Splitting details:
Query is not split
(36) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "C_CUSTOMER_SK", "C_CUSTOMER_ID"
from TPCDS.CUSTOMER
Pushdown Oracle SQL, Query Splitting details:
Query is not split
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf0b558e" class="outline-4">
<h4 id="orgf0b558e"><span class="section-number-4">4.2.2</span> Pushdown plan for Q5, Q69</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
Following are pushdown plans for couple more TPCDS queries to show the extent of
the pushdown we can do.
</p>
<ul class="org-ul">
<li>Query Q5 is a multi star schema report across Sales, Web and Catalog channels. Its SQL contains
joins, aggregates, unions, CTEs, rollup.</li>
<li>Query 69 is used to identify customers with different buying behavior in 2
separate quarters. Its SQL contains joins, aggregates, subquery predicates.</li>
</ul>

<p>
<b>TPCDS Q5 oracle pushdown:</b>
</p>
<div class="org-src-container">
<pre class="src src-text">sql(s"explain oracle pushdown $q5").show(1000, false)

|Project (1)
+- BatchScan (2)

(2) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "channel_10_sparkora", "id_6_sparkora", "sales", "returns", "profit"
from ( select "channel_10_sparkora", "id_6_sparkora", SUM("sales") AS "sales", SUM("returns") AS "returns", SUM("profit") AS "profit"
from ( select SUM("sales_price") AS "sales", SUM("return_amt") AS "returns", (cast(SUM("profit") as NUMBER(38, 17)) - cast(SUM("net_loss") as NUMBER(38, 17))) AS "profit", 'store channel' AS "channel", CONCAT('store' , "S_STORE_ID") AS "id"
from ( select "SS_STORE_SK" AS "store_sk", "SS_SOLD_DATE_SK" AS "date_sk", "SS_EXT_SALES_PRICE" AS "sales_price", "SS_NET_PROFIT" AS "profit", 0E-18 AS "return_amt", 0E-18 AS "net_loss"
from TPCDS.STORE_SALES
where ("SS_STORE_SK" IS NOT NULL AND "SS_SOLD_DATE_SK" IS NOT NULL) UNION ALL select "SR_STORE_SK" AS "store_sk", "SR_RETURNED_DATE_SK" AS "date_sk", 0E-18 AS "sales_price", 0E-18 AS "profit", "SR_RETURN_AMT" AS "return_amt", "SR_NET_LOSS" AS "net_loss"
from TPCDS.STORE_RETURNS
where ("SR_STORE_SK" IS NOT NULL AND "SR_RETURNED_DATE_SK" IS NOT NULL) )  join TPCDS.DATE_DIM  on ("date_sk" = "D_DATE_SK") join TPCDS.STORE  on ("store_sk" = "S_STORE_SK")
where (("D_DATE" IS NOT NULL AND ("D_DATE" &gt;= TRUNC(TIMESTAMP '2000-08-23 07:00:00.000000'))) AND ("D_DATE" &lt;= TRUNC(TIMESTAMP '2000-09-06 07:00:00.000000')))
group by "S_STORE_ID" UNION ALL select SUM("sales_price") AS "sales", SUM("return_amt") AS "returns", (cast(SUM("profit") as NUMBER(38, 17)) - cast(SUM("net_loss") as NUMBER(38, 17))) AS "profit", 'catalog channel' AS "channel", CONCAT('catalog_page' , "CP_CATALOG_PAGE_ID") AS "id"
from ( select "CS_CATALOG_PAGE_SK" AS "page_sk", "CS_SOLD_DATE_SK" AS "date_sk", "CS_EXT_SALES_PRICE" AS "sales_price", "CS_NET_PROFIT" AS "profit", 0E-18 AS "return_amt", 0E-18 AS "net_loss"
from TPCDS.CATALOG_SALES
where ("CS_CATALOG_PAGE_SK" IS NOT NULL AND "CS_SOLD_DATE_SK" IS NOT NULL) UNION ALL select "CR_CATALOG_PAGE_SK" AS "page_sk", "CR_RETURNED_DATE_SK" AS "date_sk", 0E-18 AS "sales_price", 0E-18 AS "profit", "CR_RETURN_AMOUNT" AS "return_amt", "CR_NET_LOSS" AS "net_loss"
from TPCDS.CATALOG_RETURNS
where ("CR_CATALOG_PAGE_SK" IS NOT NULL AND "CR_RETURNED_DATE_SK" IS NOT NULL) )  join TPCDS.DATE_DIM  on ("date_sk" = "D_DATE_SK") join TPCDS.CATALOG_PAGE  on ("page_sk" = "CP_CATALOG_PAGE_SK")
where (("D_DATE" IS NOT NULL AND ("D_DATE" &gt;= TRUNC(TIMESTAMP '2000-08-23 07:00:00.000000'))) AND ("D_DATE" &lt;= TRUNC(TIMESTAMP '2000-09-06 07:00:00.000000')))
group by "CP_CATALOG_PAGE_ID" UNION ALL select SUM("sales_price") AS "sales", SUM("return_amt") AS "returns", (cast(SUM("profit") as NUMBER(38, 17)) - cast(SUM("net_loss") as NUMBER(38, 17))) AS "profit", 'web channel' AS "channel", CONCAT('web_site' , "WEB_SITE_ID") AS "id"
from ( select "WS_WEB_SITE_SK" AS "wsr_web_site_sk", "WS_SOLD_DATE_SK" AS "date_sk", "WS_EXT_SALES_PRICE" AS "sales_price", "WS_NET_PROFIT" AS "profit", 0E-18 AS "return_amt", 0E-18 AS "net_loss"
from TPCDS.WEB_SALES
where ("WS_WEB_SITE_SK" IS NOT NULL AND "WS_SOLD_DATE_SK" IS NOT NULL) UNION ALL select "WS_WEB_SITE_SK" AS "wsr_web_site_sk", "WR_RETURNED_DATE_SK" AS "date_sk", 0E-18 AS "sales_price", 0E-18 AS "profit", "WR_RETURN_AMT" AS "return_amt", "WR_NET_LOSS" AS "net_loss"
from TPCDS.WEB_RETURNS  join TPCDS.WEB_SALES  on (("WR_ITEM_SK" = "WS_ITEM_SK") AND ("WR_ORDER_NUMBER" = "WS_ORDER_NUMBER"))
where ("WR_RETURNED_DATE_SK" IS NOT NULL AND "WS_WEB_SITE_SK" IS NOT NULL) )  join TPCDS.DATE_DIM  on ("date_sk" = "D_DATE_SK") join TPCDS.WEB_SITE  on ("wsr_web_site_sk" = "WEB_SITE_SK")
where (("D_DATE" IS NOT NULL AND ("D_DATE" &gt;= TRUNC(TIMESTAMP '2000-08-23 07:00:00.000000'))) AND ("D_DATE" &lt;= TRUNC(TIMESTAMP '2000-09-06 07:00:00.000000')))
group by "WEB_SITE_ID" )   , lateral ( select "channel" "channel_10_sparkora", "id" "id_6_sparkora", 0 "spark_grouping_id_8_sparkora" from dual union all select "channel", null, 1 from dual union all select null, null, 3 from dual )
group by "channel_10_sparkora", "id_6_sparkora", "spark_grouping_id_8_sparkora"
order by "channel_10_sparkora" ASC NULLS FIRST, "id_6_sparkora" ASC NULLS FIRST )
where rownum &lt;= 100
Pushdown Oracle SQL, Query Splitting details:
Query is not split
</pre>
</div>

<p>
<b>TPCDS Q69 oracle pushdown:</b>
</p>
<div class="org-src-container">
<pre class="src src-text">sql(s"explain oracle pushdown $q69").show(1000, false)

|Project (1)
+- BatchScan (2)

(2) BatchScan
Oracle Instance:
   DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
Pushdown Oracle SQL:
select "CD_GENDER", "CD_MARITAL_STATUS", "CD_EDUCATION_STATUS", "cnt1", "CD_PURCHASE_ESTIMATE", "cnt2", "CD_CREDIT_RATING", "cnt3"
from ( select "CD_GENDER", "CD_MARITAL_STATUS", "CD_EDUCATION_STATUS", COUNT(1) AS "cnt1", "CD_PURCHASE_ESTIMATE", COUNT(1) AS "cnt2", "CD_CREDIT_RATING", COUNT(1) AS "cnt3"
from TPCDS.CUSTOMER "sparkora_0" join TPCDS.CUSTOMER_ADDRESS  on ("C_CURRENT_ADDR_SK" = "CA_ADDRESS_SK") join TPCDS.CUSTOMER_DEMOGRAPHICS  on ("C_CURRENT_CDEMO_SK" = "CD_DEMO_SK")
where ((((("C_CURRENT_ADDR_SK" IS NOT NULL AND "C_CURRENT_CDEMO_SK" IS NOT NULL) AND  "sparkora_0"."C_CUSTOMER_SK" IN ( select "SS_CUSTOMER_SK"
from TPCDS.STORE_SALES  join TPCDS.DATE_DIM  on ("SS_SOLD_DATE_SK" = "D_DATE_SK")
where ("SS_SOLD_DATE_SK" IS NOT NULL AND (((("D_YEAR" IS NOT NULL AND "D_MOY" IS NOT NULL) AND ("D_YEAR" = 2001.000000000000000000)) AND ("D_MOY" &gt;= 4.000000000000000000)) AND ("D_MOY" &lt;= 6.000000000000000000))) )) AND not exists ( select 1
from TPCDS.WEB_SALES  join TPCDS.DATE_DIM  on ("WS_SOLD_DATE_SK" = "D_DATE_SK")
where (("WS_SOLD_DATE_SK" IS NOT NULL AND (((("D_YEAR" IS NOT NULL AND "D_MOY" IS NOT NULL) AND ("D_YEAR" = 2001.000000000000000000)) AND ("D_MOY" &gt;= 4.000000000000000000)) AND ("D_MOY" &lt;= 6.000000000000000000))) AND ("sparkora_0"."C_CUSTOMER_SK" = "WS_BILL_CUSTOMER_SK")) )) AND not exists ( select 1
from TPCDS.CATALOG_SALES  join TPCDS.DATE_DIM  on ("CS_SOLD_DATE_SK" = "D_DATE_SK")
where (("CS_SOLD_DATE_SK" IS NOT NULL AND (((("D_YEAR" IS NOT NULL AND "D_MOY" IS NOT NULL) AND ("D_YEAR" = 2001.000000000000000000)) AND ("D_MOY" &gt;= 4.000000000000000000)) AND ("D_MOY" &lt;= 6.000000000000000000))) AND ("sparkora_0"."C_CUSTOMER_SK" = "CS_SHIP_CUSTOMER_SK")) )) AND "CA_STATE" IN ( 'KY', 'GA', 'NM' ))
group by "CD_GENDER", "CD_MARITAL_STATUS", "CD_EDUCATION_STATUS", "CD_PURCHASE_ESTIMATE", "CD_CREDIT_RATING"
order by "CD_GENDER" ASC NULLS FIRST, "CD_MARITAL_STATUS" ASC NULLS FIRST, "CD_EDUCATION_STATUS" ASC NULLS FIRST, "CD_PURCHASE_ESTIMATE" ASC NULLS FIRST, "CD_CREDIT_RATING" ASC NULLS FIRST )
where rownum &lt;= 100
Pushdown Oracle SQL, Query Splitting details:
Query is not split
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf2e0e50" class="outline-3">
<h3 id="orgf2e0e50"><span class="section-number-3">4.3</span> Registration and Use of Oracle Functions(Row and Aggregate) in Spark SQL</h3>
<div class="outline-text-3" id="text-4-3">
<p>
We extend the SparkSession with a <code>registerOracleFunction</code> function. The User
can specify an existing Oracle Function in the form <code>(packageName,
functionName)</code>. In the following we show the registration and use of Oracle's
<code>STANDARD.SYS_CONTEXT</code> row and  <a href="https://asktom.oracle.com/pls/apex/f?p=100:11:0::::p11_question_id:15637744429336">STRAGG</a> aggregate functions.
</p>

<div class="org-src-container">
<pre class="src src-scala">spark.sqlContext.setConf("spark.sql.oracle.enable.pushdown", "true")

import org.apache.spark.sql.oracle._

// REGISTER ROW function
spark.registerOracleFunction(Some("STANDARD"), "SYS_CONTEXT")
sql("""
select 
  oracle.sys_context('USERENV', 'CLIENT_PROGRAM_NAME') ora_client_pgm
from sparktest.unit_test
limit 1
""".stripMargin).show(10000, false)

// REGISTER AGG function
spark.registerOracleFunction(None, "STRAGG")
sql(
"""
select c_char_5, 
       oracle.stragg(c_char_1)
from sparktest.unit_test
group by c_char_5
""".stripMargin).show(10000, false)
</pre>
</div>
</div>
</div>
<div id="outline-container-org851852d" class="outline-3">
<h3 id="org851852d"><span class="section-number-3">4.4</span> Spark SQL Macros</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-org26a6eb8" class="outline-4">
<h4 id="org26a6eb8"><span class="section-number-4">4.4.1</span> Defining Macros</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
The process of defining Spark SQL Macros is almost identical to defining custom functions in
Spark. So consider a simple function <code>(i : Int) =&gt; i + 2</code>. Following is how you would
define it as a custom Spark function and as a Spark SQL Macro.
</p>

<div class="org-src-container">
<pre class="src src-scala">spark.udf.register("intUDF", (i: Int) =&gt; {
       i + 2
      })

import org.apache.spark.sql.defineMacros._
spark.registerMacro("intUDM", spark.udm((i: Int) =&gt; {
   i + 2
  }))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd1bab67" class="outline-4">
<h4 id="orgd1bab67"><span class="section-number-4">4.4.2</span> Tax and Discount Calculation</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
Now consider a <b>tax and discount calculation</b> defined as:
</p>
<ul class="org-ul">
<li>no tax on groceries, alcohol is `10.5%`, everything else is `9.5%`</li>
<li>on Sundays give a discount of `5%` on alcohol.</li>
</ul>

<p>
This would be defined using a custom function and Spark SQL macro with the following
code.
</p>

<div class="org-src-container">
<pre class="src src-scala">/* CUSTOM FUNCTION DEFINITION */
spark.udf.register("taxAndDiscountF", {(prodCat : String, amt : Double) =&gt;
    val taxRate = prodCat match {
      case "grocery" =&gt; 0.0
      case "alcohol" =&gt; 10.5
      case _ =&gt; 9.5
    }
    val currDate = currentDate(ZoneId.systemDefault())
    val discount = if (getDayOfWeek(currDate) == 1 &amp;&amp; prodCat == "alcohol") 0.05 else 0.0

    amt * ( 1.0 - discount) * (1.0 + taxRate)
})

/* SQL MACRO DEFINITION */
import org.apache.spark.sql.defineMacros._
import org.apache.spark.sql.sqlmacros.DateTimeUtils._
import java.time.ZoneId
spark.registerMacro("taxAndDiscountM", spark.udm({(prodCat : String, amt : Double) =&gt;
    val taxRate = prodCat match {
      case "grocery" =&gt; 0.0
      case "alcohol" =&gt; 10.5
      case _ =&gt; 9.5
    }
    val currDate = currentDate(ZoneId.systemDefault())
    val discount = if (getDayOfWeek(currDate) == 1 &amp;&amp; prodCat == "alcohol") 0.05 else 0.0

    amt * ( 1.0 - discount) * (1.0 + taxRate)
}))
</pre>
</div>
</div>
</div>
<div id="outline-container-org1157398" class="outline-4">
<h4 id="org1157398"><span class="section-number-4">4.4.3</span> Query Plan and execution</h4>
<div class="outline-text-4" id="text-4-4-3">
<p>
Now consider a query  on <code>items</code> that invokes the Spark SQL Macro. We show both
the extended and formatted plans so you can see the logical plan Spark generates
and what query gets pushed to Oracle.
</p>

<div class="org-src-container">
<pre class="src src-scala">spark.sql(
  """
    |explain extended
    |select i_item_id,
    |       taxAndDiscountM(I_CATEGORY, I_CURRENT_PRICE) as taxAndDiscount
    |from item""".stripMargin
).show(1000, false)


spark.sql(
  """
    |explain formatted
    |select i_item_id,
    |       taxAndDiscountM(I_CATEGORY, I_CURRENT_PRICE) as taxAndDiscount
    |from item""".stripMargin
).show(1000, false)
</pre>
</div>

<p>
<b>The plans are:</b>
</p>

<div class="org-src-container">
<pre class="src src-text">
OUTPUT OF EXPLAIN EXTENDED:

|== Parsed Logical Plan ==
'Project ['i_item_id, 'taxAndDiscount('I_CATEGORY, 'I_CURRENT_PRICE) AS taxAndDiscount#1075]
+- 'UnresolvedRelation [item], [], false

== Analyzed Logical Plan ==
i_item_id: string, taxAndDiscount: double
Project [i_item_id#1082, ((cast(I_CURRENT_PRICE#1086 as double) * (1.0 - if (((dayofweek(current_date(Some(America/Los_Angeles))) = 1) AND (I_CATEGORY#1093 = alcohol))) 0.05 else 0.0)) * (1.0 + CASE WHEN (I_CATEGORY#1093 = grocery) THEN 0.0 WHEN (I_CATEGORY#1093 = alcohol) THEN 10.5 ELSE 9.5 END)) AS taxAndDiscount#1075]
+- SubqueryAlias oracle.tpcds.item
   +- RelationV2[I_ITEM_SK#1081, I_ITEM_ID#1082, I_REC_START_DATE#1083, I_REC_END_DATE#1084, I_ITEM_DESC#1085, I_CURRENT_PRICE#1086, I_WHOLESALE_COST#1087, I_BRAND_ID#1088, I_BRAND#1089, I_CLASS_ID#1090, I_CLASS#1091, I_CATEGORY_ID#1092, I_CATEGORY#1093, I_MANUFACT_ID#1094, I_MANUFACT#1095, I_SIZE#1096, I_FORMULATION#1097, I_COLOR#1098, I_UNITS#1099, I_CONTAINER#1100, I_MANAGER_ID#1101, I_PRODUCT_NAME#1102] TPCDS.ITEM

== Optimized Logical Plan ==
RelationV2[I_ITEM_ID#1082, taxAndDiscount#1075] TPCDS.ITEM

== Physical Plan ==
*(1) Project [I_ITEM_ID#1082, taxAndDiscount#1075]
+- BatchScan[I_ITEM_ID#1082, taxAndDiscount#1075] class org.apache.spark.sql.connector.read.oracle.OraPushdownScan
|

OUTPUT OF EXPLAIN FORMATTED:

|== Physical Plan ==
 Project (2)
+- BatchScan (1)


(1) BatchScan
Output [2]: [I_ITEM_ID#1121, taxAndDiscount#1114]
OraPlan: 00 OraSingleQueryBlock [I_ITEM_ID#1121, ((cast(I_CURRENT_PRICE#1125 as double) * 1.0) * (1.0 + CASE WHEN (I_CATEGORY#1132 = grocery) THEN 0.0 WHEN (I_CATEGORY#1132 = alcohol) THEN 10.5 ELSE 9.5 END)) AS taxAndDiscount#1114], [oracolumnref(I_ITEM_ID#1121), oraalias(((cast(I_CURRENT_PRICE#1125 as double) * 1.0) * (1.0 + CASE WHEN (I_CATEGORY#1132 = grocery) THEN 0.0 WHEN (I_CATEGORY#1132 = alcohol) THEN 10.5 ELSE 9.5 END)) AS taxAndDiscount#1114)]
01 +- OraTableScan TPCDS.ITEM, [I_ITEM_ID#1121, I_CURRENT_PRICE#1125, I_CATEGORY#1132]
ReadSchema: struct&lt;I_ITEM_ID:string,taxAndDiscount:double&gt;
dsKey: DataSourceKey(jdbc:oracle:thin:@den02ads:1531/cdb1_pdb7.regress.rdbms.dev.us.oracle.com,tpcds)
oraPushdownSQL: select "I_ITEM_ID", ((cast("I_CURRENT_PRICE" as NUMBER(30, 15)) * 1.0d) * (1.0d + CASE WHEN ("I_CATEGORY" = 'grocery') THEN 0.0d WHEN ("I_CATEGORY" = 'alcohol') THEN 10.5d ELSE 9.5d END)) AS "taxAndDiscount"
from TPCDS.ITEM

(2) Project [codegen id : 1]
Output [2]: [I_ITEM_ID#1121, taxAndDiscount#1114]
Input [2]: [I_ITEM_ID#1121, taxAndDiscount#1114]
</pre>
</div>

<p>
The analyzed catalyst expression for the`taxDiscount` invocation is below. <b>Since it
 contains no custom function invocation, it is completely pushed to Oracle</b> 
</p>
<div class="org-src-container">
<pre class="src src-text">(
  (cast(I_CURRENT_PRICE#1086 as double) *
  (1.0 - if (((dayofweek(current_date(Some(America/Los_Angeles))) = 1) AND (I_CATEGORY#1093 = alcohol))) 0.05 else 0.0)) *
  (1.0 + CASE WHEN (I_CATEGORY#1093 = grocery) THEN 0.0 WHEN (I_CATEGORY#1093 = alcohol) THEN 10.5 ELSE 9.5 END)
) AS taxAndDiscount#1075
</pre>
</div>


<p>
<b>The generated Oracle SQL is even simpler because it got generated after
constant folding was applied</b>
</p>
<div class="org-src-container">
<pre class="src src-text">(
  (cast("I_CURRENT_PRICE" as NUMBER(30, 15)) * 1.0d) *
  (1.0d + CASE WHEN ("I_CATEGORY" = 'grocery') THEN 0.0d WHEN ("I_CATEGORY" = 'alcohol') THEN 10.5d ELSE 9.5d END)
) AS "taxAndDiscount"
</pre>
</div>

<p>
<b>Contrast this with the query plan and execution if we had used the custom
 function instead</b> (you can easily try this on your own): 
</p>
<ul class="org-ul">
<li>the custom function expression wouldn't be pushable to Oracle, so raw column
values would be pulled out of Oracle.</li>
<li>Function evaluation in Spark is row by row and would incur
Serialization/Deserialization cost between catalyst and JVM values.</li>
<li>The Spark SQL Macro plan would be even more advantageous if the function was
in the <code>where clause</code></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Harish Butani</p>
<p class="date">Created: 2021-03-23 Tue 15:51</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
